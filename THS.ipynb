{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "THS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOTLUqOfwEUx85aFovfgPEx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristalys47/THS/blob/master/THS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW6aAS0pttnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(7)\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, Conv2D, Flatten, Reshape, AveragePooling2D, \\\n",
        "    Concatenate, ZeroPadding2D, Multiply\n",
        "\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "\n",
        "class  TweetSentiment2DCNN2Channel:\n",
        "    def __init__(self, max_sentence_len, embedding_builder):\n",
        "        self.max_sentence_len = max_sentence_len\n",
        "        self.embedding_builder = embedding_builder\n",
        "        self.model = None\n",
        "\n",
        "    def build(self, first_dropout=0.0, padding='same', filters=4, kernel_size=(1,1), strides=(1,1), activation='relu',\n",
        "              dense_units=64, second_dropout=0.0):\n",
        "\n",
        "        # Input Layer 1 - tweet in right order\n",
        "        sentence_input = Input(shape=(self.max_sentence_len,), name=\"INPUT_1\")\n",
        "        reverse_sentence_input = Input(shape=(self.max_sentence_len,), name=\"INPUT_2\")\n",
        "        # Embedding layer\n",
        "        embeddings_layer = self.pretrained_embedding_layer()\n",
        "        embeddings1 = embeddings_layer(sentence_input)\n",
        "        embeddings2 = embeddings_layer(reverse_sentence_input)\n",
        "\n",
        "        # Reshape\n",
        "        embeddings1= Reshape((self.max_sentence_len, self.embedding_builder.get_dimensions(), 1))(embeddings1)\n",
        "        embeddings2= Reshape((self.max_sentence_len, self.embedding_builder.get_dimensions(), 1))(embeddings2)\n",
        "\n",
        "        #stack both input to make it a 2 chanell input\n",
        "        concat_embeddings = Concatenate(axis = -1)([embeddings1, embeddings2])\n",
        "        print(\"concat_embeddings: \", concat_embeddings)\n",
        "        # Reshape with channels\n",
        "        #X = Reshape((self.max_sentence_len, self.embedding_builder.get_dimensions(), 1))(embeddings)\n",
        "\n",
        "        # First convolutional layer\n",
        "        kernel_width = self.embedding_builder.get_dimensions()\n",
        "        kernel_height = 3\n",
        "        kernel_size = (kernel_height, kernel_width)\n",
        "        X = Conv2D(filters=20, kernel_size=kernel_size, strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV2D_1\")(concat_embeddings)\n",
        "        #X  = Conv2D(filters = 66, kernel_size = (kernel_height+2, 1),  strides=(1, 1), padding='same', activation=activation,\n",
        "        #           name=\"CONV2D_2\")(X)\n",
        "        #MAX pooling\n",
        "        pool_height =  self.max_sentence_len - kernel_height + 1  # assumes zero padding and stride of 1\n",
        "        pool_size = (pool_height, 1)\n",
        "        X = AveragePooling2D(pool_size=pool_size, name = \"MAXPOOL_1\")(X)\n",
        "\n",
        "        #Flatten\n",
        "        X = Flatten()(X)\n",
        "\n",
        "        # Attention\n",
        "        #att_dense = 70*20*1\n",
        "        #attention_probs = Dense(att_dense, activation='softmax', name='attention_probs')(X)\n",
        "        #attention_mul = Multiply(name='attention_multiply')([X, attention_probs])\n",
        "\n",
        "\n",
        "        # # First dense layer\n",
        "        dense_units = 128\n",
        "        X = Dense(units=int(dense_units/2), activation='relu', name=\"DENSE_1\")(X)\n",
        "        X = Dropout(second_dropout, name=\"DROPOUT_1\")(X)\n",
        "\n",
        "        # # Second dense layer\n",
        "        X = Dense(units=dense_units, activation='relu', name=\"DENSE_2\")(X)\n",
        "        X = Dropout(second_dropout, name=\"DROPOUT_2\")(X)\n",
        "        #\n",
        "        # # Third layer\n",
        "        X = Dense(units=int(dense_units/2), activation='relu', name=\"DENSE_3\")(X)\n",
        "        X = Dropout(second_dropout, name=\"DROPOUT_3\")(X)\n",
        "\n",
        "        # Final layer\n",
        "        X = Dense(1, activation= \"sigmoid\", name=\"FINAL_SIGMOID\")(X)\n",
        "        # create the model\n",
        "        self.model = Model(input=[sentence_input, reverse_sentence_input] , output=X)\n",
        "\n",
        "    def pretrained_embedding_layer(self):\n",
        "        # create Keras embedding layer\n",
        "        word_to_idx, idx_to_word, word_embeddings = self.embedding_builder.read_embedding()\n",
        "        #vocabulary_len = len(word_to_idx) + 1\n",
        "        vocabulary_len = len(word_to_idx)\n",
        "        emb_dimension = self.embedding_builder.get_dimensions()\n",
        "        # get the matrix for the sentences\n",
        "        embedding_matrix = word_embeddings\n",
        "        #embedding_matrix = np.vstack([word_embeddings, np.zeros((vocabulary_len,))])\n",
        "\n",
        "        # embedding layer\n",
        "        embedding_layer = Embedding(input_dim=vocabulary_len, output_dim=emb_dimension, trainable=False, name=\"EMBEDDING\")\n",
        "        embedding_layer.build((None,))\n",
        "        embedding_layer.set_weights([embedding_matrix])\n",
        "        return embedding_layer\n",
        "\n",
        "    def summary(self):\n",
        "        self.model.summary()\n",
        "\n",
        "    def compile(self, loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']):\n",
        "        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    def fit(self, X, Y, epochs = 50, batch_size = 32, shuffle=True, callbacks=None, validation_split=0.0, class_weight=None):\n",
        "        return self.model.fit(X, Y, epochs=epochs, batch_size=batch_size, shuffle=shuffle, callbacks=callbacks,\n",
        "                       validation_split=validation_split, class_weight=class_weight, verbose=2)\n",
        "\n",
        "    def evaluate(self, X_test, Y_test):\n",
        "        return self.model.evaluate(X_test, Y_test)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def get_sentiment(self, prediction):\n",
        "        return np.argmax(prediction)\n",
        "\n",
        "    #def sentiment_string(self, sentiment):\n",
        "    #    return self.sentiment_map[sentiment]\n",
        "\n",
        "    def save_model(self, json_filename, h5_filename):\n",
        "        json_model = self.model.to_json()\n",
        "        with open(json_filename, \"w+\") as json_file:\n",
        "            json_file.write(json_model)\n",
        "        self.model.save_weights(h5_filename)\n",
        "        return\n",
        "\n",
        "class TweetSentiment2DCNN1x12Channel(TweetSentiment2DCNN2Channel):\n",
        "    def __init__(self, max_sentence_len, embedding_builder):\n",
        "            super().__init__(max_sentence_len, embedding_builder)\n",
        "\n",
        "    def build(self, first_dropout=0.0, padding='same', filters=4, kernel_size=(1, 1), strides=(1, 1),\n",
        "              activation='relu', dense_units=64, second_dropout=0.0):\n",
        "\n",
        "            # Input Layer 1 - tweet in right order\n",
        "        sentence_input = Input(shape=(self.max_sentence_len,), name=\"INPUT_1\")\n",
        "        reverse_sentence_input = Input(shape=(self.max_sentence_len,), name=\"INPUT_2\")\n",
        "        # Embedding layer\n",
        "        embeddings_layer = self.pretrained_embedding_layer()\n",
        "        embeddings1 = embeddings_layer(sentence_input)\n",
        "        embeddings2 = embeddings_layer(reverse_sentence_input)\n",
        "\n",
        "        # Reshape\n",
        "        embeddings1= Reshape((self.max_sentence_len, self.embedding_builder.get_dimensions(), 1))(embeddings1)\n",
        "        embeddings2= Reshape((self.max_sentence_len, self.embedding_builder.get_dimensions(), 1))(embeddings2)\n",
        "\n",
        "        #stack both input to make it a 2 chanell input\n",
        "        concat_embeddings = Concatenate(axis = -1)([embeddings1, embeddings2])\n",
        "        print(\"concat_embeddings: \", concat_embeddings)\n",
        "\n",
        "        # one by one convolution\n",
        "        onebyone = Conv2D(filters=32, kernel_size=(1,1), strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_1X1_1\")(concat_embeddings)\n",
        "        kernel_width = self.embedding_builder.get_dimensions()\n",
        "        kernel_height = 3\n",
        "        kernel_size = (kernel_height, kernel_width)\n",
        "        X = Conv2D(filters=64, kernel_size=kernel_size, strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV2D_1\")(onebyone)\n",
        "        # X  = Conv2D(filters = 66, kernel_size = (kernel_height+2, 1),  strides=(1, 1), padding='same', activation=activation,\n",
        "        #           name=\"CONV2D_2\")(X)\n",
        "        # MAX pooling\n",
        "        pool_height = self.max_sentence_len - kernel_height + 1  # assumes zero padding and stride of 1\n",
        "        pool_size = (3, 1)\n",
        "        #pool_size = (pool_height, 1)\n",
        "        X = AveragePooling2D(pool_size=pool_size, name=\"MAXPOOL_1\")(X)\n",
        "\n",
        "        #X = Conv2D(filters=1, kernel_size=(1,1), strides=(1, 1), padding=padding, activation=activation,\n",
        "        #           name=\"CONV2D_2\")(X)\n",
        "\n",
        "        # Flatten\n",
        "        X = Flatten()(X)\n",
        "\n",
        "        # # First dense layer\n",
        "        dense_units = 128\n",
        "        X = Dense(units=int(dense_units / 2), activation='relu', name=\"DENSE_1\")(X)\n",
        "        X = Dropout(second_dropout, name=\"DROPOUT_1\")(X)\n",
        "\n",
        "        # # Second dense layer\n",
        "        X = Dense(units=dense_units, activation='relu', name=\"DENSE_2\")(X)\n",
        "        X = Dropout(second_dropout, name=\"DROPOUT_2\")(X)\n",
        "        #\n",
        "        # # Third layer\n",
        "        X = Dense(units=int(dense_units / 2), activation='relu', name=\"DENSE_3\")(X)\n",
        "        X = Dropout(second_dropout, name=\"DROPOUT_3\")(X)\n",
        "\n",
        "        # Final layer\n",
        "        X = Dense(1, activation=\"sigmoid\", name=\"FINAL_SIGMOID\")(X)\n",
        "        # create the model\n",
        "        self.model = Model(input=[sentence_input, reverse_sentence_input], output=X)\n",
        "\n",
        "\n",
        "class TweetSentiment2DCNN1x12Channelv2(TweetSentiment2DCNN2Channel):\n",
        "    def __init__(self, max_sentence_len, embedding_builder):\n",
        "            super().__init__(max_sentence_len, embedding_builder)\n",
        "\n",
        "    def build(self, first_dropout=0.0, padding='same', filters=4, kernel_size=(1, 1), strides=(1, 1),\n",
        "              activation='relu', dense_units=64, second_dropout=0.0):\n",
        "\n",
        "            # Input Layer 1 - tweet in right order\n",
        "        sentence_input = Input(shape=(self.max_sentence_len,), name=\"INPUT_1\")\n",
        "        reverse_sentence_input = Input(shape=(self.max_sentence_len,), name=\"INPUT_2\")\n",
        "        # Embedding layer\n",
        "        embeddings_layer = self.pretrained_embedding_layer()\n",
        "        embeddings1 = embeddings_layer(sentence_input)\n",
        "        embeddings2 = embeddings_layer(reverse_sentence_input)\n",
        "\n",
        "        # Reshape\n",
        "        embeddings1= Reshape((self.max_sentence_len, self.embedding_builder.get_dimensions(), 1))(embeddings1)\n",
        "        embeddings2= Reshape((self.max_sentence_len, self.embedding_builder.get_dimensions(), 1))(embeddings2)\n",
        "\n",
        "        #stack both input to make it a 2 chanell input\n",
        "        concat_embeddings = Concatenate(axis = -1)([embeddings1, embeddings2])\n",
        "        print(\"concat_embeddings: \", concat_embeddings)\n",
        "\n",
        "        # one by one convolution\n",
        "        onebyone = Conv2D(filters=16, kernel_size=(1,1), strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_1X1_1\")(concat_embeddings)\n",
        "        kernel_width = self.embedding_builder.get_dimensions()\n",
        "        kernel_height = 3\n",
        "        kernel_size = (kernel_height, kernel_width)\n",
        "        X = Conv2D(filters=32, kernel_size=kernel_size, strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV2D_1\")(onebyone)\n",
        "        # X  = Conv2D(filters = 66, kernel_size = (kernel_height+2, 1),  strides=(1, 1), padding='same', activation=activation,\n",
        "        #           name=\"CONV2D_2\")(X)\n",
        "        # MAX pooling\n",
        "        pool_height = self.max_sentence_len - kernel_height + 1  # assumes zero padding and stride of 1\n",
        "        pool_size = (pool_height, 1)\n",
        "        X = AveragePooling2D(pool_size=pool_size, name=\"MAXPOOL_1\")(X)\n",
        "\n",
        "        # Flatten\n",
        "        X = Flatten()(X)\n",
        "\n",
        "        flatonebyone = Flatten()(onebyone)\n",
        "        # concact one by one with MaxPoling\n",
        "        X = Concatenate(axis=-1)([X, flatonebyone])\n",
        "        # # First dense layer\n",
        "        dense_units = 128\n",
        "        X = Dense(units=int(dense_units / 2), activation='relu', name=\"DENSE_1\")(X)\n",
        "        X = Dropout(second_dropout, name=\"DROPOUT_1\")(X)\n",
        "\n",
        "        # # Second dense layer\n",
        "        X = Dense(units=dense_units, activation='relu', name=\"DENSE_2\")(X)\n",
        "        X = Dropout(second_dropout, name=\"DROPOUT_2\")(X)\n",
        "        #\n",
        "        # # Third layer\n",
        "        X = Dense(units=int(dense_units / 2), activation='relu', name=\"DENSE_3\")(X)\n",
        "        X = Dropout(second_dropout, name=\"DROPOUT_3\")(X)\n",
        "\n",
        "        # Final layer\n",
        "        X = Dense(1, activation=\"sigmoid\", name=\"FINAL_SIGMOID\")(X)\n",
        "        # create the model\n",
        "        self.model = Model(inputs=[sentence_input, reverse_sentence_input], outputs=X)\n",
        "\n",
        "class TweetSentimentInception(TweetSentiment2DCNN2Channel):\n",
        "    def __init__(self, max_sentence_len, embedding_builder):\n",
        "            super().__init__(max_sentence_len, embedding_builder)\n",
        "\n",
        "    def build(self, first_dropout=0.0, padding='same', filters=4, kernel_size=(1, 1), strides=(1, 1),\n",
        "              activation='relu', dense_units=64, second_dropout=0.0):\n",
        "\n",
        "        # Input Layer 1 - tweet in right order\n",
        "        sentence_input = Input(shape=(self.max_sentence_len,), name=\"INPUT_1\")\n",
        "        reverse_sentence_input = Input(shape=(self.max_sentence_len,), name=\"INPUT_2\")\n",
        "        # Embedding layer\n",
        "        embeddings_layer = self.pretrained_embedding_layer()\n",
        "        embeddings1 = embeddings_layer(sentence_input)\n",
        "        embeddings2 = embeddings_layer(reverse_sentence_input)\n",
        "\n",
        "        # Reshape\n",
        "        embeddings1= Reshape((self.max_sentence_len, self.embedding_builder.get_dimensions(), 1))(embeddings1)\n",
        "        embeddings2= Reshape((self.max_sentence_len, self.embedding_builder.get_dimensions(), 1))(embeddings2)\n",
        "\n",
        "        #stack both input to make it a 2 chanell input\n",
        "        concat_embeddings = Concatenate(axis = -1)([embeddings1, embeddings2])\n",
        "        print(\"concat_embeddings: \", concat_embeddings)\n",
        "\n",
        "        #compute 1x1 convolution on input\n",
        "        onebyone = Conv2D(filters=filters, kernel_size=(1,1), strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_1X1_1\")(concat_embeddings)\n",
        "\n",
        "\n",
        "        #compute 3xdimension convolution on one by one\n",
        "        kernel_width = self.embedding_builder.get_dimensions()\n",
        "        kernel_height = 3\n",
        "        kernel_size = (kernel_height, kernel_width)\n",
        "        threebydim1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_3xdim_1\")(onebyone)\n",
        "\n",
        "        #compute 3xdimension convolution on input\n",
        "        kernel_width = self.embedding_builder.get_dimensions()\n",
        "        kernel_height = 3\n",
        "        kernel_size = (kernel_height, kernel_width)\n",
        "        threebydim2 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_3xdim_2\")(concat_embeddings)\n",
        "\n",
        "        #compute 5xdimension convolution on one by one\n",
        "        kernel_width = self.embedding_builder.get_dimensions()\n",
        "        kernel_height = 5\n",
        "        kernel_size = (kernel_height, kernel_width)\n",
        "        fivebydim1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_5xdim_1\")(onebyone)\n",
        "        fivebydim1 = ZeroPadding2D((1, 0))(fivebydim1)\n",
        "\n",
        "        #compute 5xdimension convolution on input\n",
        "        kernel_width = self.embedding_builder.get_dimensions()\n",
        "        kernel_height = 5\n",
        "        kernel_size = (kernel_height, kernel_width)\n",
        "        fivebydim2 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_5xdim_2\")(concat_embeddings)\n",
        "        fivebydim2 = ZeroPadding2D((1,0))(fivebydim2)\n",
        "\n",
        "        concat_layer = Concatenate(axis = -1)([threebydim1, threebydim2,fivebydim1, fivebydim2])\n",
        "\n",
        "        final_onebyone = Conv2D(filters=1, kernel_size=(1,1), strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_1X1_final\")(concat_layer)\n",
        "\n",
        "        #final_onebyone = MaxPooling2D((2,1))(final_onebyone)\n",
        "        #final_onebyone = AveragePooling2D((2,1))(final_onebyone)\n",
        "\n",
        "        # Flatten\n",
        "        X = Flatten()(final_onebyone)\n",
        "        #X = Dropout(0.10, name=\"DROPOUT_1\")(X)\n",
        "\n",
        "        # attention\n",
        "        att_dense = 70\n",
        "        attention_probs = Dense(att_dense, activation='softmax', name='attention_probs')(X)\n",
        "        attention_mul = Multiply(name='attention_multiply')([X, attention_probs])\n",
        "\n",
        "        X = Dense(units=int(dense_units / 1), activation='relu', name=\"DENSE_1\")(attention_mul)\n",
        "        X = Dense(units=int(dense_units / 2), activation='relu', name=\"DENSE_2\")(X)\n",
        "        X = Dense(units=int(dense_units / 4), activation='relu', name=\"DENSE_3\")(X)\n",
        "\n",
        "        # Final layer\n",
        "        #X = Dense(1, activation=\"sigmoid\", name=\"FINAL_SIGMOID\")(X)\n",
        "        X = Dense(3, activation=\"softmax\", name=\"FINAL_SOFTMAX\")(X)\n",
        "        # create the model\n",
        "        self.model = Model(input=[sentence_input, reverse_sentence_input], output=X)\n",
        "\n",
        "\n",
        "\n",
        "class TweetSentimentInceptionOneChan(TweetSentiment2DCNN2Channel):\n",
        "    def __init__(self, max_sentence_len, embedding_builder):\n",
        "            super().__init__(max_sentence_len, embedding_builder)\n",
        "\n",
        "    def build(self, first_dropout=0.0, padding='same', filters=4, kernel_size=(1, 1), strides=(1, 1),\n",
        "              activation='relu', dense_units=64, second_dropout=0.0):\n",
        "\n",
        "        # Input Layer 1 - tweet in right order\n",
        "        sentence_input = Input(shape=(self.max_sentence_len,), name=\"INPUT_1\")\n",
        "        #reverse_sentence_input = Input(shape=(self.max_sentence_len,), name=\"INPUT_2\")\n",
        "        # Embedding layer\n",
        "        embeddings_layer = self.pretrained_embedding_layer()\n",
        "        embeddings1 = embeddings_layer(sentence_input)\n",
        "        #embeddings2 = embeddings_layer(reverse_sentence_input)\n",
        "\n",
        "        # Reshape\n",
        "        embeddings1= Reshape((self.max_sentence_len, self.embedding_builder.get_dimensions(), 1))(embeddings1)\n",
        "        #embeddings2= Reshape((self.max_sentence_len, self.embedding_builder.get_dimensions(), 1))(embeddings2)\n",
        "\n",
        "        #stack both input to make it a 2 chanell input\n",
        "        #concat_embeddings = Concatenate(axis = -1)([embeddings1, embeddings2])\n",
        "        #print(\"concat_embeddings: \", concat_embeddings)\n",
        "\n",
        "        #compute 1x1 convolution on input\n",
        "        onebyone = Conv2D(filters=filters, kernel_size=(1,1), strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_1X1_1\")(embeddings1)\n",
        "\n",
        "\n",
        "        #compute 3xdimension convolution on one by one\n",
        "        kernel_width = self.embedding_builder.get_dimensions()\n",
        "        kernel_height = 3\n",
        "        kernel_size = (kernel_height, kernel_width)\n",
        "        threebydim1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_3xdim_1\")(onebyone)\n",
        "\n",
        "        #compute 3xdimension convolution on input\n",
        "        kernel_width = self.embedding_builder.get_dimensions()\n",
        "        kernel_height = 3\n",
        "        kernel_size = (kernel_height, kernel_width)\n",
        "        threebydim2 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_3xdim_2\")(embeddings1)\n",
        "\n",
        "        #compute 5xdimension convolution on one by one\n",
        "        kernel_width = self.embedding_builder.get_dimensions()\n",
        "        kernel_height = 5\n",
        "        kernel_size = (kernel_height, kernel_width)\n",
        "        fivebydim1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_5xdim_1\")(onebyone)\n",
        "        fivebydim1 = ZeroPadding2D((1, 0))(fivebydim1)\n",
        "\n",
        "        #compute 5xdimension convolution on input\n",
        "        kernel_width = self.embedding_builder.get_dimensions()\n",
        "        kernel_height = 5\n",
        "        kernel_size = (kernel_height, kernel_width)\n",
        "        fivebydim2 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_5xdim_2\")(embeddings1)\n",
        "        fivebydim2 = ZeroPadding2D((1,0))(fivebydim2)\n",
        "\n",
        "        concat_layer = Concatenate(axis = -1)([threebydim1, threebydim2,fivebydim1, fivebydim2])\n",
        "        #final_onebyone = AveragePooling2D((2,1))(concat_layer)\n",
        "        final_onebyone = Conv2D(filters=filters*2, kernel_size=(1,1), strides=(1, 1), padding=padding, activation=activation,\n",
        "                   name=\"CONV_1X1_final\")(concat_layer)\n",
        "\n",
        "        #final_onebyone = MaxPooling2D((2,1))(final_onebyone)\n",
        "        #final_onebyone = AveragePooling2D((2,1))(final_onebyone)\n",
        "\n",
        "        # Flatten\n",
        "        X = Flatten()(final_onebyone)\n",
        "        #X = Flatten()(concat_layer)\n",
        "\n",
        "        #X = Dropout(0.10, name=\"DROPOUT_1\")(X)\n",
        "\n",
        "        # attention\n",
        "        # att_dense = 70\n",
        "        # attention_probs = Dense(att_dense, activation='softmax', name='attention_probs')(X)\n",
        "        # attention_mul = Multiply(name='attention_multiply')([X, attention_probs])\n",
        "        #\n",
        "        # X = Dense(units=int(dense_units / 1), activation='relu', name=\"DENSE_1\")(attention_mul)\n",
        "        #X = Dense(units=int(dense_units / 2), activation='relu', name=\"DENSE_2\")(X)\n",
        "        #X = Dense(units=int(dense_units / 4), activation='relu', name=\"DENSE_3\")(X)\n",
        "\n",
        "        X = Dense(units=128, activation='relu', name=\"DENSE_2\")(X)\n",
        "        X = Dense(units=64, activation='relu', name=\"DENSE_3\")(X)\n",
        "        X = Dense(units=32, activation='relu', name=\"DENSE_4\")(X)\n",
        "\n",
        "        # Final layer\n",
        "        #X = Dense(1, activation=\"sigmoid\", name=\"FINAL_SIGMOID\")(X)\n",
        "        X = Dense(3, activation=\"softmax\", name=\"FINAL_SOFTMAX\")(X)\n",
        "        # create the model\n",
        "        self.model = Model(inputs=[sentence_input], outputs=X)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE54ugg3nP-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class EmbeddingException(Exception):\n",
        "    pass\n",
        "\n",
        "class GloveEmbedding:\n",
        "    def __init__(self, filename, dimensions =50):\n",
        "        if not filename :\n",
        "            raise Exception(\"Illegal file name.\")\n",
        "        if dimensions < 1:\n",
        "            raise Exception(\"Illegal value for dimensions\")\n",
        "\n",
        "        self.filename = filename\n",
        "        self.dimensions = dimensions\n",
        "\n",
        "    def get_dimensions(self):\n",
        "        return self.dimensions\n",
        "\n",
        "    def parse_embedding(self, data_vector):\n",
        "\n",
        "        result = []\n",
        "        for n in data_vector:\n",
        "            result.append(float(n))\n",
        "        return result\n",
        "\n",
        "    def read_embedding_bad(self):\n",
        "        try:\n",
        "            data_in = open(self.filename, \"r\")\n",
        "        except Exception as e:\n",
        "            msg  = sys.exc_info()[0]\n",
        "            raise EmbeddingException(msg) from e\n",
        "        else:\n",
        "            i = 0\n",
        "            word_to_idx = {}\n",
        "            idx_to_word = {}\n",
        "            word_to_vect = []\n",
        "            with data_in:\n",
        "                for line in enumerate(data_in):\n",
        "                    #print(line)\n",
        "                    parts = line[1].split()\n",
        "                    word_part = parts[0]\n",
        "                    vector_parts = parts[1:]\n",
        "                    idx_to_word[i] = word_part\n",
        "                    word_to_idx[word_part] = i\n",
        "                    i = i+ 1\n",
        "                    word_to_vect.append(self.parse_embedding(vector_parts))\n",
        "            #add <unk> token\n",
        "            #unk = np.random.rand(self.dimensions,)\n",
        "            unk = np.ones((self.dimensions,))\n",
        "            idx_to_word[i] = \"<unk>\"\n",
        "            word_to_idx[\"<unk>\"] = i\n",
        "            word_to_vect.append(unk)\n",
        "            np_word_to_vect = np.array(word_to_vect)\n",
        "            return word_to_idx, idx_to_word, np_word_to_vect\n",
        "\n",
        "    def read_embedding(self):\n",
        "        try:\n",
        "            data_in = open(self.filename, \"r\",  encoding='utf-8')\n",
        "        except Exception as e:\n",
        "            msg  = sys.exc_info()[0]\n",
        "            raise EmbeddingException(msg) from e\n",
        "        else:\n",
        "            i = 1\n",
        "            word_to_idx = {}\n",
        "            word_to_idx['<EOF>'] = 0\n",
        "            idx_to_word = {}\n",
        "            idx_to_word[0] = None\n",
        "            word_to_vect = []\n",
        "            word_to_vect.append(np.zeros((self.dimensions,)))\n",
        "            with data_in:\n",
        "                for line in enumerate(data_in):\n",
        "                    #print(line)\n",
        "                    parts = line[1].split()\n",
        "                    word_part = parts[0]\n",
        "                    vector_parts = parts[1:]\n",
        "                    idx_to_word[i] = word_part\n",
        "                    word_to_idx[word_part] = i\n",
        "                    i = i+ 1\n",
        "                    word_to_vect.append(self.parse_embedding(vector_parts))\n",
        "            #add <unk> token\n",
        "            #unk = np.random.rand(self.dimensions,)\n",
        "            unk = np.ones((self.dimensions,))\n",
        "            idx_to_word[i] = \"<unk>\"\n",
        "            word_to_idx[\"<unk>\"] = i\n",
        "            word_to_vect.append(unk)\n",
        "            np_word_to_vect = np.array(word_to_vect)\n",
        "            return word_to_idx, idx_to_word, np_word_to_vect\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Word2VecEmbedding:\n",
        "    def __init__(self, filename, dimensions =50):\n",
        "        if not filename :\n",
        "            raise Exception(\"Illegal file name.\")\n",
        "        if dimensions < 1:\n",
        "            raise Exception(\"Illegal value for dimensions\")\n",
        "\n",
        "        self.filename = filename\n",
        "        self.dimensions = dimensions\n",
        "\n",
        "    def get_dimensions(self):\n",
        "        return self.dimensions\n",
        "\n",
        "    def parse_embedding(self, data_vector):\n",
        "\n",
        "        result = []\n",
        "        for n in data_vector:\n",
        "            result.append(float(n))\n",
        "        return result\n",
        "\n",
        "    def read_embedding_bad(self):\n",
        "        try:\n",
        "            data_in = open(self.filename, \"r\")\n",
        "        except Exception as e:\n",
        "            msg  = sys.exc_info()[0]\n",
        "            raise EmbeddingException(msg) from e\n",
        "        else:\n",
        "            i = 0\n",
        "            word_to_idx = {}\n",
        "            idx_to_word = {}\n",
        "            word_to_vect = []\n",
        "            with data_in:\n",
        "                for line in enumerate(data_in):\n",
        "                    #print(line)\n",
        "                    parts = line[1].split()\n",
        "                    word_part = parts[0]\n",
        "                    vector_parts = parts[1:]\n",
        "                    idx_to_word[i] = word_part\n",
        "                    word_to_idx[word_part] = i\n",
        "                    i = i+ 1\n",
        "                    word_to_vect.append(self.parse_embedding(vector_parts))\n",
        "            #add <unk> token\n",
        "            unk = np.random.rand(self.dimensions,)\n",
        "            idx_to_word[i] = \"<unk>\"\n",
        "            word_to_idx[\"<unk>\"] = i\n",
        "            word_to_vect.append(unk)\n",
        "            np_word_to_vect = np.array(word_to_vect)\n",
        "            return word_to_idx, idx_to_word, np_word_to_vect\n",
        "\n",
        "    def read_embedding(self):\n",
        "        try:\n",
        "            data_in = open(self.filename, \"r\")\n",
        "        except Exception as e:\n",
        "            msg  = sys.exc_info()[0]\n",
        "            raise EmbeddingException(msg) from e\n",
        "        else:\n",
        "            i = 1\n",
        "            word_to_idx = {}\n",
        "            word_to_idx['<EOF>'] = 0\n",
        "            idx_to_word = {}\n",
        "            idx_to_word[0] = None\n",
        "            word_to_vect = []\n",
        "            word_to_vect.append(np.zeros((self.dimensions,)))\n",
        "            with data_in:\n",
        "                for line in enumerate(data_in):\n",
        "                    #print(line)\n",
        "                    parts = line[1].split()\n",
        "                    word_part = parts[0]\n",
        "                    vector_parts = parts[1:]\n",
        "                    idx_to_word[i] = word_part\n",
        "                    word_to_idx[word_part] = i\n",
        "                    i = i+ 1\n",
        "                    word_to_vect.append(self.parse_embedding(vector_parts))\n",
        "            #add <unk> token\n",
        "            unk = np.random.rand(self.dimensions,)\n",
        "            idx_to_word[i] = \"<unk>\"\n",
        "            word_to_idx[\"<unk>\"] = i\n",
        "            word_to_vect.append(unk)\n",
        "            np_word_to_vect = np.array(word_to_vect)\n",
        "            return word_to_idx, idx_to_word, np_word_to_vect\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcF8srL-nOgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPSILON_VALUE = 0.000000001\n",
        "\n",
        "class UnkWords:\n",
        "    def __init__(self, word_to_idx):\n",
        "        pass\n",
        "\n",
        "class SentenceToIndices:\n",
        "\n",
        "    def __init__(self, word_to_idx):\n",
        "        self.word_to_idx = word_to_idx\n",
        "\n",
        "    def map_sentence(self, sentence):\n",
        "        result = []\n",
        "        words = sentence.split()\n",
        "        for w in words:\n",
        "            if w in self.word_to_idx:\n",
        "                result.append(self.word_to_idx[w])\n",
        "            else:\n",
        "                result.append(self.word_to_idx[\"<unk>\"])\n",
        "        return result\n",
        "\n",
        "    def map_sentence_list(self, sentence_list):\n",
        "        result = []\n",
        "        max_len = 0\n",
        "        counter_len = 0.0\n",
        "        total_len = 0.0\n",
        "        for s in sentence_list:\n",
        "            mapped = self.map_sentence(s)\n",
        "            mapped_len = len(mapped)\n",
        "            counter_len = counter_len + 1\n",
        "            total_len = total_len + mapped_len\n",
        "            if mapped_len > max_len:\n",
        "                max_len = mapped_len\n",
        "                max_s = s\n",
        "            result.append(mapped)\n",
        "        print(\"max_len: \", max_len)\n",
        "        print(\"avg_len: \", total_len/counter_len)\n",
        "        return result, max_len\n",
        "\n",
        "class PadSentences:\n",
        "    def __init__(self, max_len):\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def pad(self, sentence):\n",
        "        padding_len = self.max_len - len(sentence)\n",
        "        padding = []\n",
        "        if (padding_len > 0 ):\n",
        "            r = range(0, padding_len)\n",
        "            for _  in r:\n",
        "                padding.append(0)\n",
        "        return sentence + padding\n",
        "\n",
        "    def pad_list(self, sentence_list):\n",
        "        result = []\n",
        "        for s in sentence_list:\n",
        "            result.append(self.pad(s))\n",
        "        return result\n",
        "\n",
        "\n",
        "class SentenceToEmbedding:\n",
        "    def __init__(self, word_to_idx, idx_to_word, word_to_vect):\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_word = idx_to_word\n",
        "        self.word_to_vect = word_to_vect\n",
        "\n",
        "    def map_sentence(self, sentence, max_len = 0):\n",
        "        S = SentenceToIndices(self.word_to_idx)\n",
        "        matrix = None\n",
        "        mapped_sentence = S.map_sentence(sentence)\n",
        "        for i in mapped_sentence:\n",
        "            e = self.word_to_vect[i]\n",
        "            if matrix is None:\n",
        "                matrix = np.array(e)\n",
        "            else:\n",
        "                matrix = np.vstack([matrix, e])\n",
        "        if max_len > 0:\n",
        "            padding_len = max_len - len(mapped_sentence)\n",
        "            #print(\"max_len: \", max_len)\n",
        "            #print(\"len(mapped_sentence): \", len(mapped_sentence))\n",
        "            #print(\"padding: \", padding_len)\n",
        "            if padding_len > 0:\n",
        "                shape = matrix[0].shape\n",
        "                zero_vector = np.zeros(shape)\n",
        "                for _ in range(0, padding_len):\n",
        "                    matrix = np.vstack([matrix, zero_vector])\n",
        "        return matrix\n",
        "\n",
        "class SentenceToEmbeddingWithEPSILON:\n",
        "    def __init__(self, word_to_idx, idx_to_word, word_to_vect):\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_word = idx_to_word\n",
        "        self.word_to_vect = word_to_vect\n",
        "\n",
        "    def map_sentence(self, sentence, max_len = 0):\n",
        "        S = SentenceToIndices(self.word_to_idx)\n",
        "        matrix = None\n",
        "        mapped_sentence = S.map_sentence(sentence)\n",
        "        for i in mapped_sentence:\n",
        "            e = self.word_to_vect[i]\n",
        "            if matrix is None:\n",
        "                matrix = np.array(e)\n",
        "            else:\n",
        "                matrix = np.vstack([matrix, e])\n",
        "        if max_len > 0:\n",
        "            padding_len = max_len - len(mapped_sentence)\n",
        "            #print(\"max_len: \", max_len)\n",
        "            #print(\"len(mapped_sentence): \", len(mapped_sentence))\n",
        "            #print(\"padding: \", padding_len)\n",
        "            if padding_len > 0:\n",
        "                shape = matrix[0].shape\n",
        "                if (len(mapped_sentence) == 1):\n",
        "                    shape = (50,)\n",
        "                zero_vector = np.ones(shape) * EPSILON_VALUE\n",
        "                for _ in range(0, padding_len):\n",
        "                    matrix = np.vstack([matrix, zero_vector])\n",
        "\n",
        "        return matrix\n",
        "\n",
        "class TrimSentences:\n",
        "    def __init__(self, trim_size):\n",
        "        self.trim_size = trim_size\n",
        "\n",
        "    def trim(self, sentence):\n",
        "        if len(sentence) > self.trim_size:\n",
        "            return sentence[:self.trim_size]\n",
        "        else:\n",
        "            return sentence\n",
        "\n",
        "    def trim_list(self, sentence_list):\n",
        "        result = []\n",
        "        for s in sentence_list:\n",
        "            temp = self.trim(s)\n",
        "            result.append(temp)\n",
        "        return result\n",
        "import numpy as np\n",
        "\n",
        "EPSILON_VALUE = 0.000000001\n",
        "\n",
        "class UnkWords:\n",
        "    def __init__(self, word_to_idx):\n",
        "        pass\n",
        "\n",
        "class SentenceToIndices:\n",
        "\n",
        "    def __init__(self, word_to_idx):\n",
        "        self.word_to_idx = word_to_idx\n",
        "\n",
        "    def map_sentence(self, sentence):\n",
        "        result = []\n",
        "        words = sentence.split()\n",
        "        for w in words:\n",
        "            if w in self.word_to_idx:\n",
        "                result.append(self.word_to_idx[w])\n",
        "            else:\n",
        "                result.append(self.word_to_idx[\"<unk>\"])\n",
        "        return result\n",
        "\n",
        "    def map_sentence_list(self, sentence_list):\n",
        "        result = []\n",
        "        max_len = 0\n",
        "        counter_len = 0.0\n",
        "        total_len = 0.0\n",
        "        for s in sentence_list:\n",
        "            mapped = self.map_sentence(s)\n",
        "            mapped_len = len(mapped)\n",
        "            counter_len = counter_len + 1\n",
        "            total_len = total_len + mapped_len\n",
        "            if mapped_len > max_len:\n",
        "                max_len = mapped_len\n",
        "                max_s = s\n",
        "            result.append(mapped)\n",
        "        print(\"max_len: \", max_len)\n",
        "        print(\"avg_len: \", total_len/counter_len)\n",
        "        return result, max_len\n",
        "\n",
        "class PadSentences:\n",
        "    def __init__(self, max_len):\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def pad(self, sentence):\n",
        "        padding_len = self.max_len - len(sentence)\n",
        "        padding = []\n",
        "        if (padding_len > 0 ):\n",
        "            r = range(0, padding_len)\n",
        "            for _  in r:\n",
        "                padding.append(0)\n",
        "        return sentence + padding\n",
        "\n",
        "    def pad_list(self, sentence_list):\n",
        "        result = []\n",
        "        for s in sentence_list:\n",
        "            result.append(self.pad(s))\n",
        "        return result\n",
        "\n",
        "\n",
        "class SentenceToEmbedding:\n",
        "    def __init__(self, word_to_idx, idx_to_word, word_to_vect):\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_word = idx_to_word\n",
        "        self.word_to_vect = word_to_vect\n",
        "\n",
        "    def map_sentence(self, sentence, max_len = 0):\n",
        "        S = SentenceToIndices(self.word_to_idx)\n",
        "        matrix = None\n",
        "        mapped_sentence = S.map_sentence(sentence)\n",
        "        for i in mapped_sentence:\n",
        "            e = self.word_to_vect[i]\n",
        "            if matrix is None:\n",
        "                matrix = np.array(e)\n",
        "            else:\n",
        "                matrix = np.vstack([matrix, e])\n",
        "        if max_len > 0:\n",
        "            padding_len = max_len - len(mapped_sentence)\n",
        "            #print(\"max_len: \", max_len)\n",
        "            #print(\"len(mapped_sentence): \", len(mapped_sentence))\n",
        "            #print(\"padding: \", padding_len)\n",
        "            if padding_len > 0:\n",
        "                shape = matrix[0].shape\n",
        "                zero_vector = np.zeros(shape)\n",
        "                for _ in range(0, padding_len):\n",
        "                    matrix = np.vstack([matrix, zero_vector])\n",
        "        return matrix\n",
        "\n",
        "class SentenceToEmbeddingWithEPSILON:\n",
        "    def __init__(self, word_to_idx, idx_to_word, word_to_vect):\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_word = idx_to_word\n",
        "        self.word_to_vect = word_to_vect\n",
        "\n",
        "    def map_sentence(self, sentence, max_len = 0):\n",
        "        S = SentenceToIndices(self.word_to_idx)\n",
        "        matrix = None\n",
        "        mapped_sentence = S.map_sentence(sentence)\n",
        "        for i in mapped_sentence:\n",
        "            e = self.word_to_vect[i]\n",
        "            if matrix is None:\n",
        "                matrix = np.array(e)\n",
        "            else:\n",
        "                matrix = np.vstack([matrix, e])\n",
        "        if max_len > 0:\n",
        "            padding_len = max_len - len(mapped_sentence)\n",
        "            #print(\"max_len: \", max_len)\n",
        "            #print(\"len(mapped_sentence): \", len(mapped_sentence))\n",
        "            #print(\"padding: \", padding_len)\n",
        "\n",
        "            if padding_len > 0:\n",
        "                shape = matrix[0].shape\n",
        "                if(len(mapped_sentence)==1):\n",
        "                    shape=(50, )\n",
        "                zero_vector = np.ones(shape) * EPSILON_VALUE\n",
        "                for _ in range(0, padding_len):\n",
        "                    matrix = np.vstack([matrix, zero_vector])\n",
        "        return matrix\n",
        "\n",
        "class TrimSentences:\n",
        "    def __init__(self, trim_size):\n",
        "        self.trim_size = trim_size\n",
        "\n",
        "    def trim(self, sentence):\n",
        "        if len(sentence) > self.trim_size:\n",
        "            return sentence[:self.trim_size]\n",
        "        else:\n",
        "            return sentence\n",
        "\n",
        "    def trim_list(self, sentence_list):\n",
        "        result = []\n",
        "        for s in sentence_list:\n",
        "            temp = self.trim(s)\n",
        "            result.append(temp)\n",
        "        return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcbQ-fSHmCgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "class ErrorAnalysis:\n",
        "    @staticmethod\n",
        "    def store_errors(X, Y, Y_Pred, file_name):\n",
        "        errors = Y != Y_Pred\n",
        "        errors = errors * 1 # trick to convert bool to 0s and 1s\n",
        "        i = 0\n",
        "        with open(\"/content/drive/My Drive/Kristalys/\" + file_name, \"w\") as f:\n",
        "            out_f = csv.writer(f, delimiter=' ')\n",
        "            for e in errors:\n",
        "                if e:\n",
        "                    data = []\n",
        "                    tweet = X[i]\n",
        "                    label = Y[i]\n",
        "                    pred = Y_Pred[i]\n",
        "                    data.append(tweet)\n",
        "                    data.append(label)\n",
        "                    data.append(pred)\n",
        "                    out_f.writerow(data)\n",
        "\n",
        "                i = i + 1\n",
        "            f.flush()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N53ciVUSkWyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "\n",
        "\n",
        "    #predicted positives\n",
        "    predictions = K.round(y_pred)\n",
        "    predicted_positives = K.sum(predictions)\n",
        "\n",
        "    #true positives\n",
        "    true_positives = K.sum(K.round(y_true * predictions))\n",
        "    P = true_positives / (predicted_positives + K.epsilon())\n",
        "    return P\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "\n",
        "\n",
        "    #predicted positives\n",
        "    predictions = K.round(y_pred)\n",
        "\n",
        "    #all positives\n",
        "    all_positives = K.sum(y_true)\n",
        "\n",
        "    #true positives\n",
        "    true_positives = K.sum(K.round(y_true * predictions))\n",
        "\n",
        "    R = true_positives / all_positives\n",
        "    return R\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "\n",
        "\n",
        "    P = precision(y_true, y_pred)\n",
        "    R = recall(y_true, y_pred)\n",
        "    return 2*((P*R)/(P+R+K.epsilon()))\n",
        "\n",
        "\n",
        "# def f1(y_true, y_pred):\n",
        "#     def recall(y_true, y_pred):\n",
        "#         \"\"\"Recall metric.\n",
        "#\n",
        "#         Only computes a batch-wise average of recall.\n",
        "#\n",
        "#         Computes the recall, a metric for multi-label classification of\n",
        "#         how many relevant items are selected.\n",
        "#         \"\"\"\n",
        "#         print(\"y_true \", y_true.eval())\n",
        "#         print(\"y_pred \", y_pred.eval())\n",
        "#\n",
        "#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#         possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "#         recall = true_positives / (possible_positives + K.epsilon())\n",
        "#         return recall\n",
        "#\n",
        "#     def precision(y_true, y_pred):\n",
        "#         \"\"\"Precision metric.\n",
        "#\n",
        "#         Only computes a batch-wise average of precision.\n",
        "#\n",
        "#         Computes the precision, a metric for multi-label classification of\n",
        "#         how many selected items are relevant.\n",
        "#         \"\"\"\n",
        "#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#         predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "#         precision = true_positives / (predicted_positives + K.epsilon())\n",
        "#         return precision\n",
        "#     precision = precision(y_true, y_pred)\n",
        "#     recall = recall(y_true, y_pred)\n",
        "#     return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "#\n",
        "#\n",
        "# def recall(y_true, y_pred):\n",
        "#     \"\"\"Recall metric.\n",
        "#\n",
        "#     Only computes a batch-wise average of recall.\n",
        "#\n",
        "#     Computes the recall, a metric for multi-label classification of\n",
        "#     how many relevant items are selected.\n",
        "#     \"\"\"\n",
        "#     print(\"y_true \", y_true)\n",
        "#     print(\"y_pred \", y_pred)\n",
        "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "#     recall = true_positives / (possible_positives + K.epsilon())\n",
        "#     return recall\n",
        "#\n",
        "# def precision(y_true, y_pred):\n",
        "#     \"\"\"Precision metric.\n",
        "#\n",
        "#     Only computes a batch-wise average of precision.\n",
        "#\n",
        "#     Computes the precision, a metric for multi-label classification of\n",
        "#     how many selected items are relevant.\n",
        "#     \"\"\"\n",
        "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
        "#     return precision\n",
        "#\n",
        "# def tprate(y_true, y_pred):\n",
        "#     return recall(y_true, y_pred)\n",
        "#\n",
        "# def fprate2(y_true, y_pred):\n",
        "#     #invert true and negative so negative is 1  and true is 1.\n",
        "#     y_true = 1 - y_true\n",
        "#     #invert predictions so that we get 1 for the predictions originally set to 0\n",
        "#     y_pred = 1 - y_pred\n",
        "#     return recall(y_true, y_pred)\n",
        "\n",
        "def fprate(y_true, y_pred):\n",
        "    #predicted positives\n",
        "    predictions = K.round(y_pred)\n",
        "    predicted_positives = K.sum(predictions)\n",
        "\n",
        "    #all positives\n",
        "    all_positives = K.sum(y_true)\n",
        "\n",
        "    #true positives\n",
        "    true_positives = K.sum(K.round(y_true * predictions))\n",
        "\n",
        "    false_positive = predicted_positives - true_positives\n",
        "\n",
        "    #negatives\n",
        "    y_false = 1 - y_true\n",
        "\n",
        "    all_negatives = K.sum(y_false)\n",
        "    fpr = false_positive / (all_negatives + K.epsilon())\n",
        "    return fpr\n",
        "\n",
        "\n",
        "# def fprate(y_true, y_pred):\n",
        "#     # true positives\n",
        "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "#     # predicted_positives = true_positives + false_positives\n",
        "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "#     # false_positives\n",
        "#     false_positive = predicted_positives - true_positives\n",
        "#     # Now work on negatives\n",
        "#     y_false = 1 - y_true\n",
        "#     possible_negatives = K.sum(K.round(K.clip(y_false, 0, 1)))\n",
        "#     fprate = false_positive / (possible_negatives  + K.epsilon())\n",
        "#     return fprate\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    n_samples = len(y_pred)\n",
        "    correct = 1 * (y_true == y_pred)\n",
        "    return sum(correct) / n_samples\n",
        "\n",
        "\n",
        "def calculate_cm_metrics(c_matrix, track):\n",
        "    prec_0 = c_matrix[0][0] / (c_matrix[0][0] + c_matrix[1][0] + c_matrix[2][0])\n",
        "    prec_1 = c_matrix[1][1] / (c_matrix[1][1] + c_matrix[0][1] + c_matrix[2][1])\n",
        "    prec_2 = c_matrix[2][2] / (c_matrix[2][2] + c_matrix[1][2] + c_matrix[0][2])\n",
        "\n",
        "    recall_0 = c_matrix[0][0] / (c_matrix[0][0] + c_matrix[0][1] + c_matrix[0][2])\n",
        "    recall_1 = c_matrix[1][1] / (c_matrix[1][1] + c_matrix[1][0] + c_matrix[1][2])\n",
        "    recall_2 = c_matrix[2][2] / (c_matrix[2][2] + c_matrix[2][0] + c_matrix[2][1])\n",
        "\n",
        "    f1_0 = 2 * ((prec_0 * recall_0) / (prec_0 + recall_0))\n",
        "    f1_1 = 2 * ((prec_1 * recall_1) / (prec_1 + recall_1))\n",
        "    f1_2 = 2 * ((prec_2 * recall_2) / (prec_2 + recall_2))\n",
        "\n",
        "    tn_0 = c_matrix[1][1] + c_matrix[1][2] + c_matrix[2][1] + c_matrix[2][2]\n",
        "    tn_1 = c_matrix[0][0] + c_matrix[0][2] + c_matrix[2][0] + c_matrix[2][2]\n",
        "    tn_2 = c_matrix[0][0] + c_matrix[0][1] + c_matrix[1][0] + c_matrix[1][1]\n",
        "\n",
        "    spec_0 = tn_0 / (tn_0 + c_matrix[1][0] + c_matrix[2][0])\n",
        "    spec_1 = tn_1 / (tn_1 + c_matrix[0][1] + c_matrix[2][1])\n",
        "    spec_2 = tn_2 / (tn_2 + c_matrix[0][2] + c_matrix[1][2])\n",
        "\n",
        "    t = track + (\"Precision 0: {}\\n\" \n",
        "                \"Precision 1: {}\\n\"\n",
        "                \"Precision 2: {}\\n\"\n",
        "                \"Recall 0: {}\\n\"\n",
        "                \"Recall 1: {}\\n\"\n",
        "                \"Recall 2: {}\\n\"\n",
        "                \"F1 Score 0: {}\\n\"\n",
        "                \"F1 Score 1: {}\\n\"\n",
        "                \"F1 Score 2: {}\\n\"\n",
        "                \"Specificity 0: {}\\n\"\n",
        "                \"Specificity 1: {}\\n\"\n",
        "                \"Specificity 2: {}\\n\").format(prec_0, prec_1, prec_2, recall_0, recall_1, recall_2, f1_0, f1_1,\n",
        "                                                f1_2, spec_0, spec_1, spec_2)\n",
        "    return prec_1, recall_1, f1_1, spec_1, t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ap43an9ko8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "\n",
        "\n",
        "class ProcessTweetsCNN:\n",
        "    def __init__(self, labeled_tweets_filename, embedding_filename):\n",
        "        self.labeled_tweets_filename = labeled_tweets_filename\n",
        "        self.embedding_filename = embedding_filename\n",
        "\n",
        "    def plot(self, history):\n",
        "        # summarize history for accuracy\n",
        "        plt.figure(1)\n",
        "        plt.plot(history.history['acc'])\n",
        "        plt.plot(history.history['val_acc'])\n",
        "        plt.title('model accuracy')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        # summarize history for loss\n",
        "        plt.figure(2)\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('model loss')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.legend(['train', 'test'], loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "    def process(self, json_filename, h5_filename, plot=False, epochs = 100, vect_dimensions = 50):\n",
        "        # open the file with tweets\n",
        "        X_all = []\n",
        "        Y_all = []\n",
        "        All  = []\n",
        "\n",
        "        #with open(self.labeled_tweets_filename, \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "        with open(self.labeled_tweets_filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            i = 0\n",
        "            csv_file = csv.reader(f, delimiter=',')\n",
        "            ones_count = 0\n",
        "\n",
        "            for r in csv_file:\n",
        "                if i != 0:\n",
        "                    All.append(r)\n",
        "                i = i + 1\n",
        "\n",
        "        np.random.shuffle(All)\n",
        "\n",
        "        ones_count = 0\n",
        "        two_count = 0\n",
        "        zero_count = 0\n",
        "        for r in All:\n",
        "            tweet = r[0]\n",
        "            label = int(r[1])\n",
        "            if (label == 0):\n",
        "                zero_count += 1\n",
        "            elif (label == 1):\n",
        "                ones_count += 1\n",
        "            else:\n",
        "                two_count += 1\n",
        "            # if (label == 2):\n",
        "            #     label = 0\n",
        "            # if (label == 1) and (ones_count <= 4611):\n",
        "            #     X_all.append(tweet)\n",
        "            #     Y_all.append(label)\n",
        "            #     ones_count +=1\n",
        "            # elif (label == 0):\n",
        "            X_all.append(tweet)\n",
        "            Y_all.append(label)\n",
        "\n",
        "        print(\"len(Y_all): \", len(Y_all))\n",
        "        class_weight_val = class_weight.compute_class_weight('balanced', np.unique(Y_all), Y_all)\n",
        "        print(\"classes: \", np.unique(Y_all))\n",
        "        print(\"counts for 0, 1, 2: \", zero_count, ones_count, two_count)\n",
        "        print(\"class weight_val: \", class_weight_val)\n",
        "        class_weight_dictionary = {0: class_weight_val[0], 1: class_weight_val[1], 2: class_weight_val[2]}\n",
        "        print(\"dict: \", class_weight_dictionary)\n",
        "\n",
        "        print(\"Data Ingested\")\n",
        "        # divide the data into training and test\n",
        "        num_data = len(X_all)\n",
        "        limit = math.ceil(num_data * 0.80)\n",
        "        X_train_sentences = X_all\n",
        "        Y_train = Y_all\n",
        "        # Divide after conversions\n",
        "        # divide the data into X_train, Y_train, X_test, Y_test\n",
        "        #X_train_sentences = X_all[0: limit]\n",
        "        #Y_train = Y_all[0: limit]\n",
        "        #X_test_sentences = X_all[limit:]\n",
        "        #Y_test = Y_all[limit:]\n",
        "        #print(\"Data Divided\")\n",
        "\n",
        "        #Get embeeding\n",
        "        #G = Word2VecEmbedding(self.embedding_filename, dimensions=vect_dimensions)\n",
        "\n",
        "        G = GloveEmbedding(self.embedding_filename, dimensions=50)\n",
        "        word_to_idx, idx_to_word, embedding = G.read_embedding()\n",
        "        S = SentenceToIndices(word_to_idx)\n",
        "        X_train_indices, max_len  = S.map_sentence_list(X_train_sentences)\n",
        "        print(\"Train data mappend to indices\")\n",
        "        if max_len % 2 !=0:\n",
        "            max_len = max_len + 1\n",
        "\n",
        "        P = PadSentences(max_len)\n",
        "        X_train_pad = P.pad_list(X_train_indices)\n",
        "        print(\"Train data padded\")\n",
        "        # TRIM\n",
        "        trim_size = max_len\n",
        "        #trim_size = 33\n",
        "        Trim = TrimSentences(trim_size)\n",
        "        X_train_pad = Trim.trim_list(X_train_pad)\n",
        "        print(\"X[0], \", X_train_pad[0])\n",
        "        #convert to numPY arrays\n",
        "        X_train = np.array(X_train_pad)\n",
        "        Y_train = np.array(Y_train)\n",
        "        ones_count = np.count_nonzero(Y_train)\n",
        "        zeros_count = len(Y_train) - ones_count\n",
        "        print(\"ones count: \", ones_count)\n",
        "        print(\"zeros count: \", zeros_count)\n",
        "        print(\"two count: \", two_count)\n",
        "        Y_train_old = Y_train\n",
        "        Y_train = to_categorical(Y_train, num_classes=3)\n",
        "\n",
        "        # Divide the data\n",
        "        X_test_text = X_all[limit:]\n",
        "        X_test = X_train[limit:]\n",
        "        Y_test = Y_train[limit:]\n",
        "        X_train = X_train[0: limit]\n",
        "        Y_train = Y_train[0: limit]\n",
        "        print (\"data divided on value: \", limit)\n",
        "        print(\"lengths X_train, Y_train: \", len(X_train), len(Y_train))\n",
        "        print(\"lengths X_test, Y_test: \", len(X_test), len(Y_test))\n",
        "\n",
        "        print(\"Train data convert to numpy arrays\")\n",
        "        #NN = TweetSentiment2DCNN(trim_size, G)\n",
        "        #NN = TweetSentiment2LSTM2Dense(trim_size, G)\n",
        "        #NN =TweetSentiment2LSTM2Dense3Layer(trim_size, G)\n",
        "        #NN =TweetSentiment2LSTM2Dense4Layer(trim_size, G)\n",
        "        NN = TweetSentimentInceptionOneChan(trim_size, G)\n",
        "        #NN = TweetSentimentCNN(trim_size, G)\n",
        "        #print(\"Build GRU\")\n",
        "        #NN = TweetSentimentGRUSM(max_len, G)\n",
        "\n",
        "        print(\"model created\")\n",
        "        kernel_regularizer = l2(0.001)\n",
        "        #kernel_regularizer = None\n",
        "        NN.build(filters=11, first_dropout=0, second_dropout=0.05, padding='valid', dense_units=16)\n",
        "\n",
        "        #NN.build(first_layer_units = max_len, second_layer_units = max_len, relu_dense_layer=16, dense_layer_units = 3,\n",
        "        #         first_layer_dropout=0, second_layer_dropout=0, third_layer_dropout=0)\n",
        "        print(\"model built\")\n",
        "        NN.summary()\n",
        "        sgd = SGD(lr=0.03, momentum=0.009, decay=0.001, nesterov=True)\n",
        "        rmsprop = RMSprop(decay=0.003)\n",
        "        adam = Adam(lr=0.1, decay=0.05)\n",
        "        sgd = SGD(lr=0.05)\n",
        "        NN.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy', precision, recall, f1, fprate])\n",
        "        print(\"model compiled\")\n",
        "        print(\"Begin training\")\n",
        "        callback = TensorBoard(log_dir=\"/tmp/logs\")\n",
        "        #class_weight = {0: 0.67, 1: 0.33}\n",
        "        #class_weight = None\n",
        "        history = NN.fit(X_train, Y_train, epochs=epochs, batch_size=32, callbacks=[callback], class_weight=class_weight_dictionary)\n",
        "        print(\"Model trained\")\n",
        "        print(\"Predicting\")\n",
        "        print(\"len(X_test): \", X_test)\n",
        "        preds = NN.predict(X_test)\n",
        "        print(\"len(preds): \", len(preds))\n",
        "        print(\"type preds: \", type(preds))\n",
        "        print(\"preds before: \", preds)\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "        print(\"preds: \", preds)\n",
        "        print(\"len(preds): \", len(preds))\n",
        "        Y_test = Y_train_old[limit:]\n",
        "        print(\"Y test: \", Y_test)\n",
        "        c_matrix = confusion_matrix(Y_test, preds)\n",
        "        print(\"matrix: \", c_matrix)\n",
        "        print(\"Storing Errors: \")\n",
        "        ErrorAnalysis.store_errors(X_test_text, Y_test, preds, \"errorcnn.csv\")\n",
        "        print(\"Errors stored\")\n",
        "        print(\"Confusion matrix: \")\n",
        "        prec_1, recall_1, f1_1, spec_1, t = calculate_cm_metrics(c_matrix, '')\n",
        "        print(\"C1-> presicion, recall, F1: \", prec_1, recall_1, f1_1)\n",
        "\n",
        "        #\n",
        "        # X_test_indices, max_len = S.map_sentence_list(X_test_sentences)\n",
        "        # print(\"Test data mapped\")\n",
        "        # X_test_pad = P.pad_list(X_test_indices)\n",
        "        # print(\"Test data padded\")\n",
        "        # X_test = np.array(X_test_pad)\n",
        "        # Y_test = np.array(Y_test)\n",
        "        # print(\"Test data converted to numpy arrays\")\n",
        "        # loss, acc = NN.evaluate(X_test, Y_test, callbacks=[callback])\n",
        "        # print(\"accuracy: \", acc)\n",
        "        T = \"I have a bad case of vomit\"\n",
        "        X_Predict = [\"my zika is bad\", \"i love colombia\", \"my has been tested for ebola\", \"there is a diarrhea outbreak in the city\"]\n",
        "        X_Predict_Idx, max_len2 = S.map_sentence_list(X_Predict)\n",
        "        i =0\n",
        "        for s in X_Predict_Idx:\n",
        "            print(str(i)+ \": \", s)\n",
        "            i = i + 1\n",
        "        print(X_Predict)\n",
        "        X_Predict_Final = P.pad_list(X_Predict_Idx)\n",
        "        X_Predict_Final = Trim.trim_list(X_Predict_Final)\n",
        "        #X_Predict = [X_Predict]\n",
        "        X_Predict_Final = np.array(X_Predict_Final)\n",
        "        print(\"Predict: \", np.argmax(NN.predict(X_Predict_Final)))\n",
        "        print(\"Storing model and weights\")\n",
        "        NN.save_model(json_filename, h5_filename)\n",
        "        if plot:\n",
        "            print(\"Ploting\")\n",
        "            self.plot(history)\n",
        "        print(\"Done!\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCUq5BR5knJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fbc89597-dd14-4921-da0d-71a9a8295ffe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    print(\"Working:\")\n",
        "    #P = ProcessTweetsWord2VecOnePass2DCNNv2_1(\"data/cleantextlabels3.csv\", \"trained/embedding3.csv\")\n",
        "    #P = ProcessTweetsCNN('/content/drive/My Drive/Kristalys/cleantextlabels7.csv', '/content/drive/My Drive/Kristalys/glove.6B.50d.txt')\n",
        "    P = ProcessTweetsCNN('/content/drive/My Drive/Kristalys/BaseDeDatosVS2.csv', '/content/drive/My Drive/Kristalys/glove.6B.50d.txt')\n",
        "    #P = ProcessTweetsWord2VecTwoPassLSTMv2_1(\"data/cleantextlabels4.csv\", \"trained/embedding3-50d.csv\")\n",
        "\n",
        "    #Bueno el model12cnnv2\n",
        "    # Excelente el de modellstmatt1 con attention\n",
        "    # El mejor fue modellstmatt2 con attention\n",
        "    # also good modellstmatt3\n",
        "    # el 4 con dropout\n",
        "    # 2, 3 y 4 son buenos\n",
        "    P.process(\"modelcnnincepw6.json\", \"modelcnnincepw6.h5\", plot=False, epochs=20)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0g_X3HpnrkC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16814500-76e7-4e29-8eda-7d2776a11158"
      },
      "source": [
        "main()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello Google Drive!Working:\n",
            "len(Y_all):  12500\n",
            "classes:  [0 1 2]\n",
            "counts for 0, 1, 2:  3930 7744 826\n",
            "class weight_val:  [1.06022053 0.53805096 5.04439064]\n",
            "dict:  {0: 1.0602205258693809, 1: 0.5380509641873278, 2: 5.044390637610976}\n",
            "Data Ingested\n",
            "max_len:  72\n",
            "avg_len:  34.21232\n",
            "Train data mappend to indices\n",
            "Train data padded\n",
            "X[0],  [140125, 6284, 84, 393, 39180, 6, 276379, 33, 37, 501, 10252, 1175, 26, 45878, 6, 88243, 5, 16243, 6, 56199, 42, 304, 82, 5, 347, 13, 21, 15, 37, 198, 393, 720, 15, 1192, 5, 400001, 84, 21, 15, 52, 393, 1974, 3881, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "ones count:  8570\n",
            "zeros count:  3930\n",
            "two count:  826\n",
            "data divided on value:  10000\n",
            "lengths X_train, Y_train:  10000 10000\n",
            "lengths X_test, Y_test:  2500 2500\n",
            "Train data convert to numpy arrays\n",
            "model created\n",
            "model built\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "INPUT_1 (InputLayer)            (None, 72)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "EMBEDDING (Embedding)           (None, 72, 50)       20000100    INPUT_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 72, 50, 1)    0           EMBEDDING[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "CONV_1X1_1 (Conv2D)             (None, 72, 50, 11)   22          reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "CONV_5xdim_1 (Conv2D)           (None, 68, 1, 11)    30261       CONV_1X1_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "CONV_5xdim_2 (Conv2D)           (None, 68, 1, 11)    2761        reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "CONV_3xdim_1 (Conv2D)           (None, 70, 1, 11)    18161       CONV_1X1_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "CONV_3xdim_2 (Conv2D)           (None, 70, 1, 11)    1661        reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_5 (ZeroPadding2D (None, 70, 1, 11)    0           CONV_5xdim_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_6 (ZeroPadding2D (None, 70, 1, 11)    0           CONV_5xdim_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 70, 1, 44)    0           CONV_3xdim_1[0][0]               \n",
            "                                                                 CONV_3xdim_2[0][0]               \n",
            "                                                                 zero_padding2d_5[0][0]           \n",
            "                                                                 zero_padding2d_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "CONV_1X1_final (Conv2D)         (None, 70, 1, 22)    990         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 1540)         0           CONV_1X1_final[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "DENSE_2 (Dense)                 (None, 128)          197248      flatten_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "DENSE_3 (Dense)                 (None, 64)           8256        DENSE_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "DENSE_4 (Dense)                 (None, 32)           2080        DENSE_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "FINAL_SOFTMAX (Dense)           (None, 3)            99          DENSE_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 20,261,639\n",
            "Trainable params: 261,539\n",
            "Non-trainable params: 20,000,100\n",
            "__________________________________________________________________________________________________\n",
            "model compiled\n",
            "Begin training\n",
            "Epoch 1/20\n",
            " - 27s - loss: 0.9876 - acc: 0.5625 - precision: 0.6282 - recall: 0.3145 - f1: 0.4052 - fprate: 0.0298\n",
            "Epoch 2/20\n",
            " - 28s - loss: 0.8629 - acc: 0.6546 - precision: 0.8410 - recall: 0.5226 - f1: 0.6410 - fprate: 0.0500\n",
            "Epoch 3/20\n",
            " - 29s - loss: 0.8088 - acc: 0.6597 - precision: 0.7782 - recall: 0.5629 - f1: 0.6500 - fprate: 0.0816\n",
            "Epoch 4/20\n",
            " - 28s - loss: 0.7277 - acc: 0.6800 - precision: 0.7498 - recall: 0.6122 - f1: 0.6724 - fprate: 0.1025\n",
            "Epoch 5/20\n",
            " - 28s - loss: 0.6238 - acc: 0.7238 - precision: 0.7791 - recall: 0.6693 - f1: 0.7189 - fprate: 0.0949\n",
            "Epoch 6/20\n",
            " - 28s - loss: 0.5048 - acc: 0.7778 - precision: 0.8124 - recall: 0.7356 - f1: 0.7714 - fprate: 0.0847\n",
            "Epoch 7/20\n",
            " - 28s - loss: 0.3652 - acc: 0.8334 - precision: 0.8548 - recall: 0.8096 - f1: 0.8313 - fprate: 0.0688\n",
            "Epoch 8/20\n",
            " - 28s - loss: 0.2553 - acc: 0.8792 - precision: 0.8916 - recall: 0.8641 - f1: 0.8774 - fprate: 0.0525\n",
            "Epoch 9/20\n",
            " - 28s - loss: 0.1796 - acc: 0.9140 - precision: 0.9219 - recall: 0.9078 - f1: 0.9147 - fprate: 0.0384\n",
            "Epoch 10/20\n",
            " - 28s - loss: 0.1272 - acc: 0.9406 - precision: 0.9442 - recall: 0.9358 - f1: 0.9399 - fprate: 0.0276\n",
            "Epoch 11/20\n",
            " - 28s - loss: 0.1208 - acc: 0.9469 - precision: 0.9491 - recall: 0.9443 - f1: 0.9467 - fprate: 0.0252\n",
            "Epoch 12/20\n",
            " - 28s - loss: 0.1099 - acc: 0.9538 - precision: 0.9566 - recall: 0.9503 - f1: 0.9534 - fprate: 0.0215\n",
            "Epoch 13/20\n",
            " - 28s - loss: 0.0761 - acc: 0.9656 - precision: 0.9668 - recall: 0.9638 - f1: 0.9653 - fprate: 0.0166\n",
            "Epoch 14/20\n",
            " - 28s - loss: 0.0318 - acc: 0.9893 - precision: 0.9894 - recall: 0.9892 - f1: 0.9893 - fprate: 0.0053\n",
            "Epoch 15/20\n",
            " - 28s - loss: 0.0140 - acc: 0.9956 - precision: 0.9956 - recall: 0.9956 - f1: 0.9956 - fprate: 0.0022\n",
            "Epoch 16/20\n",
            " - 28s - loss: 0.0051 - acc: 0.9988 - precision: 0.9988 - recall: 0.9988 - f1: 0.9988 - fprate: 6.0000e-04\n",
            "Epoch 17/20\n",
            " - 28s - loss: 0.0060 - acc: 0.9990 - precision: 0.9990 - recall: 0.9990 - f1: 0.9990 - fprate: 5.0000e-04\n",
            "Epoch 18/20\n",
            " - 28s - loss: 0.0033 - acc: 0.9993 - precision: 0.9993 - recall: 0.9993 - f1: 0.9993 - fprate: 3.5000e-04\n",
            "Epoch 19/20\n",
            " - 28s - loss: 0.0496 - acc: 0.9820 - precision: 0.9827 - recall: 0.9819 - f1: 0.9823 - fprate: 0.0086\n",
            "Epoch 20/20\n",
            " - 28s - loss: 0.1605 - acc: 0.9362 - precision: 0.9388 - recall: 0.9328 - f1: 0.9357 - fprate: 0.0303\n",
            "Model trained\n",
            "Predicting\n",
            "len(X_test):  [[    21     55     31 ...      0      0      0]\n",
            " [   121      8   2583 ...      0      0      0]\n",
            " [  2682   2656    196 ...      0      0      0]\n",
            " ...\n",
            " [197849      8  25764 ...      0      0      0]\n",
            " [  3786   3449   4314 ...      0      0      0]\n",
            " [     7      1   1419 ...      0      0      0]]\n",
            "len(preds):  2500\n",
            "type preds:  <class 'numpy.ndarray'>\n",
            "preds before:  [[9.1404670e-01 8.4338479e-02 1.6148553e-03]\n",
            " [4.7562011e-02 9.2453229e-01 2.7905678e-02]\n",
            " [1.1459681e-05 9.9970621e-01 2.8227293e-04]\n",
            " ...\n",
            " [4.7437198e-04 9.9951744e-01 8.1789140e-06]\n",
            " [1.4541180e-04 9.9929595e-01 5.5858691e-04]\n",
            " [2.5625355e-03 9.9741995e-01 1.7446622e-05]]\n",
            "preds:  [0 1 1 ... 1 1 1]\n",
            "len(preds):  2500\n",
            "Y test:  [0 1 1 ... 1 1 0]\n",
            "matrix:  [[ 451  268   39]\n",
            " [ 138 1324  100]\n",
            " [  42  112   26]]\n",
            "Storing Errors: \n",
            "Errors stored\n",
            "Confusion matrix: \n",
            "C1-> presicion, recall, F1:  0.7769953051643192 0.8476312419974392 0.8107777097366811\n",
            "max_len:  8\n",
            "avg_len:  5.25\n",
            "0:  [193, 172344, 15, 979]\n",
            "1:  [42, 836, 2765]\n",
            "2:  [193, 32, 52, 3495, 11, 19128]\n",
            "3:  [64, 15, 8, 16243, 3854, 7, 1, 116]\n",
            "['my zika is bad', 'i love colombia', 'my has been tested for ebola', 'there is a diarrhea outbreak in the city']\n",
            "Predict:  3\n",
            "Storing model and weights\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}